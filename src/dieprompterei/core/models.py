"""
Pydantic models for Die Prompterei.

All data structures use strict typing and validation.
"""

from datetime import datetime
from pathlib import Path
from typing import Any, Literal

from pydantic import BaseModel, Field


class OutputSchema(BaseModel):
    """JSON schema for structured task outputs."""

    type: str
    properties: dict[str, Any]
    required: list[str] | None = None


class ValidationConfig(BaseModel):
    """Configuration for validation dataset."""

    path: Path


class MetricsConfig(BaseModel):
    """Metrics to compute during evaluation."""

    primary: str
    secondary: list[str] = Field(default_factory=list)


class ScoringConfig(BaseModel):
    """
    Scoring formula configuration.

    Formula: final_score = primary_metric - (variance_penalty_weight * normalized_variance)
    """

    base_metric_weight: float = 1.0
    variance_penalty_weight: float = 0.2


class ConvergenceConfig(BaseModel):
    """
    Convergence detection configuration.

    Optimization stops when:
    1. current_round >= min_rounds AND
    2. (improvement < improvement_threshold OR no improvement for plateau_rounds)
    """

    improvement_threshold: float = 0.02
    plateau_rounds: int = 2
    min_rounds: int = 3


class OptimizationConfig(BaseModel):
    """Optimization loop configuration."""

    max_rounds: int = 6
    candidates_per_round: int = 4
    poet_temperature: float = 0.5
    convergence: ConvergenceConfig


class LLMConfig(BaseModel):
    """LLM provider configuration."""

    provider: Literal["anthropic", "openai", "gemini"] = "anthropic"
    model: str = "claude-sonnet-4"
    judge_temperature: float = 0.2
    guardian_temperature: float = 0.3


class TaskDefinition(BaseModel):
    """Complete task specification."""

    name: str
    type: Literal["classification", "extraction", "qa", "generation"]
    goal: str
    output_schema: OutputSchema | None = None
    validation: ValidationConfig
    metrics: MetricsConfig
    scoring: ScoringConfig
    optimization: OptimizationConfig
    llm: LLMConfig
    baseline_prompt: str | None = None


class TaskConfig(BaseModel):
    """Root configuration object loaded from YAML."""

    task: TaskDefinition


class PromptCandidate(BaseModel):
    """A single prompt candidate generated by the Poet."""

    id: int
    prompt: str
    reasoning: str


class EvaluationScores(BaseModel):
    """Scores from evaluation."""

    primary: float
    variance: float
    secondary: dict[str, float] = Field(default_factory=dict)


class IndividualResult(BaseModel):
    """Result for a single validation example."""

    input: str
    predicted: dict[str, Any]
    expected: dict[str, Any]
    correct: bool
    error_type: str | None = None


class JudgeOutput(BaseModel):
    """Complete output from Judge evaluation."""

    scores: EvaluationScores
    individual_results: list[IndividualResult]
    error_analysis: list[str]
    suggestions: list[str]


class GuardianOutput(BaseModel):
    """Sanitized feedback from Guardian (no validation data leakage)."""

    scores: EvaluationScores
    error_patterns: list[str]
    suggestions: list[str]


class RoundReceipt(BaseModel):
    """Receipt for a single optimization round."""

    round: int
    prompt: str
    scores: EvaluationScores
    feedback_summary: str
    duration_seconds: float
    api_calls: int
    tokens_used: dict[str, int]
    improvement: float | None = None


class OptimizationReceipt(BaseModel):
    """Complete optimization history."""

    task_name: str
    timestamp: str
    rounds: list[RoundReceipt]
    final_prompt: str
    final_score: float
    convergence_reason: str
    total_cost: dict[str, Any]  # tokens, time, API calls

    @staticmethod
    def create_timestamp() -> str:
        """Generate ISO 8601 timestamp."""
        return datetime.now().isoformat()


class LLMResponse(BaseModel):
    """Response from an LLM API call."""

    text: str
    tokens_input: int
    tokens_output: int
